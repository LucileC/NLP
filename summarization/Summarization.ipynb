{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "UNK = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 128\n",
    "HIDDEN_SIZE = 256\n",
    "VOC_SIZE = 50000 #vocab.n_words#50000 #for both source and target\n",
    "OUTPUT_SIZE = VOC_SIZE #10 # ?? \n",
    "BASELINE_VOC_SIZE = VOC_SIZE\n",
    "MAX_LENGTH = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def loadDataset(path,limit=100000000000):\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = list()\n",
    "    i = 0\n",
    "    for line in open(path, 'r'):\n",
    "        all_data = json.loads(line)\n",
    "        for data in all_data:\n",
    "            if i <limit:\n",
    "                dataset.append(data)\n",
    "                i += 1\n",
    "            else:\n",
    "                break\n",
    "    print(\"Loaded %d pieces of data\"%len(dataset))\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Loaded 14261 pieces of data\n",
      "Loading dataset...\n",
      "Loaded 114091 pieces of data\n",
      "Loading dataset...\n",
      "Loaded 3 pieces of data\n"
     ]
    }
   ],
   "source": [
    "path_dev = \"data/msmarco_2wellformed/dev_v2.0_well_formed.json\"\n",
    "path_train = \"data/msmarco_2wellformed/train_v2.0_well_formed.json\"\n",
    "path_eval = \"data/msmarco_2wellformed/evalpublicwellformed.json\"\n",
    "dataset_dev = loadDataset(path_dev)\n",
    "dataset_train = loadDataset(path_train)\n",
    "dataset_eval = loadDataset(path_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "what are the four houses at hogwarts school of witchcraft and wizardry\n",
      "The four houses at Hogwarts School of Witchcraft and Wizardry are Godric Gryffindor, Helga Hufflepuff, Rowena Ravenclaw and Salazar Slytherin.\n",
      "\n",
      "what forms does an independent contractor need to fill out\n",
      "An independent contractor needs to fill out IRS Form W-9.\n",
      "\n",
      "where is porto alegre\n",
      "Porto Alegre is in Brazil.\n",
      "\n",
      "what is the first established state in us\n",
      "Delaware is the first established state in United States.\n",
      "\n",
      "what is true about atp molecule\n",
      "Adenosine Tri Phosphate is a molecule with a lot of chemical energy.\n",
      "\n",
      "average time for finishing london marathon\n",
      "The average time for the finishing London Marathon is 4 hours, 6 minutes and 8 seconds.\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print('')\n",
    "    x = random.choice(dataset_train)\n",
    "    print(x['query'])\n",
    "    print(x['wellFormedAnswers'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'can', \"n't\", 'open', 'the', 'door', 'because', 'the', '30-year', 'old', '/', 'blond-hair', 'guy', 'does', \"n't\", 'want', 'to', 'let', 'me', 'in', '.', 'he', \"'s\", 'mean', ',', 'is', \"n't\", 'he', '?', 'i', 'can', \"n't\", 'go', 'in', '!', 'there', \"'s\", 'no', 'other', 'way', '!']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def insertCharIfSeq(sentence1,c,seq):\n",
    "    i = 0\n",
    "    indexes = [m.start() for m in re.finditer(seq, sentence1)]\n",
    "    for index in indexes:\n",
    "        sentence1 = sentence1[:index+i] + c + sentence1[index+i:]\n",
    "        i += 1\n",
    "    return sentence1\n",
    "\n",
    "def processContractions(sentence1):\n",
    "    sentence1 = insertCharIfSeq(sentence1,\" \",\"'s\")\n",
    "    sentence1 = insertCharIfSeq(sentence1,\" \",\"'m\")\n",
    "    sentence1 = insertCharIfSeq(sentence1,\" \",\"'ll\")\n",
    "    sentence1 = insertCharIfSeq(sentence1,\" \",\"'ve\")\n",
    "    sentence1 = insertCharIfSeq(sentence1,\" \",\"'re\")\n",
    "    sentence1 = insertCharIfSeq(sentence1,\" \",\"'d\")\n",
    "    return sentence1\n",
    "\n",
    "\n",
    "def processNegatives(sentence1):\n",
    "    i = 0\n",
    "    indexes = [m.start() for m in re.finditer(\"can't\", sentence1)]\n",
    "    for index in indexes:\n",
    "        sentence1 = sentence1[:index+i+3] + sentence1[index+i+2:]\n",
    "        i += 1\n",
    "    return insertCharIfSeq(sentence1,\" \",\"n't\")\n",
    "\n",
    "## WHAT TO DO WITH HYPHENS ??\n",
    "\n",
    "def tokenizeSentence(sentence1):\n",
    "#     processFinalPeriod(sentence1)\n",
    "    sentence1 = processContractions(sentence1)\n",
    "    sentence1 = processNegatives(sentence1)\n",
    "    s = sentence1.lower()\n",
    "    s = re.sub('''([.,!\"?$;:/#`()])''', r' \\1 ', s)\n",
    "    s = re.sub('\\s{2,}', ' ', s)\n",
    "    s = s.split()\n",
    "#     s = processHyphenIfUnknownWords(s,glove)\n",
    "#     s.append('</s>')\n",
    "#     s = ['<s>'] + s\n",
    "    return s\n",
    "\n",
    "def testTokenizeSentence():\n",
    "    sentence1 = \"I can't open the door because the 30-year old/blond-hair guy doesn't want to let me in. He's mean, isn't he? I can't go in! There's no other way!\"\n",
    "    words = tokenizeSentence(sentence1)\n",
    "    print(words)\n",
    "    \n",
    "testTokenizeSentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self,voc_size=VOC_SIZE):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"UNK\"}\n",
    "        self.wordsOrderedByFreq = list()\n",
    "        self.word2count[\"UNK\"] = 0\n",
    "        self.n_words = 3\n",
    "        self.voc_size = voc_size\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence:\n",
    "            self.addWord(word)\n",
    "            \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "#             self.index2word[self.n_words] = word\n",
    "#             self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def buildVocabulary(self):\n",
    "        # sort words by frequencies\n",
    "        self.wordsOrderedByFreq = list(reversed(sorted(self.word2count, key=lambda key: self.word2count[key])))\n",
    "        # get voc_size th element in list (= last word in vocab)\n",
    "        last_word = self.wordsOrderedByFreq[self.voc_size]\n",
    "        # get frequency of last word\n",
    "        freq = self.word2count[last_word]\n",
    "        print(\"Last word in vocabulary will be %s with a frequency of appearance of %d\"%(last_word,freq))\n",
    "        for i in range(self.voc_size):\n",
    "            word = self.wordsOrderedByFreq[i]\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        for j in range(self.voc_size,len(self.wordsOrderedByFreq)):\n",
    "            self.word2index[word] = 2\n",
    "            self.n_words += 1\n",
    "            self.word2count['UNK'] += 1\n",
    "        print(\"Vocabulary (size %d) is built. The firsts words are:\"%self.voc_size)\n",
    "        for k in range(2,7):\n",
    "            word = self.index2word[k]\n",
    "            print(\"%s (appeared %d times)\"%(word,self.word2count[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n",
      "Number of different words: 664062\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "vocab = Vocab()\n",
    "\n",
    "def tokenizeDataset(dataset):\n",
    "    tokenized_dataset = list()\n",
    "    len_dataset = len(dataset)\n",
    "    print_every = math.floor(len_dataset / 10)\n",
    "    print('Tokenizing dataset...')\n",
    "    for i, data in enumerate(dataset):\n",
    "        tokenized_data = dict()\n",
    "        tokenized_data['answers'] = list()\n",
    "        tokenized_data['wellFormedAnswers'] = list()\n",
    "        tokenized_data['passages'] = list()\n",
    "        for answer in data['answers']:\n",
    "            t = tokenizeSentence(answer)\n",
    "            tokenized_data['answers'].append(t)\n",
    "            vocab.addSentence(t)\n",
    "        for wf_answer in data['wellFormedAnswers']:\n",
    "            t = tokenizeSentence(wf_answer)\n",
    "            tokenized_data['wellFormedAnswers'].append(t)\n",
    "            vocab.addSentence(t)\n",
    "        for passage in data['passages']:\n",
    "            t = tokenizeSentence(passage['passage_text'])\n",
    "            tokenized_data['passages'].append(t)\n",
    "            vocab.addSentence(t)\n",
    "        if i>0 and i%print_every == 0:\n",
    "            print('... %d%%'%(i/print_every*10),end=\"\\r\")\n",
    "        tokenized_dataset.append(tokenized_data)\n",
    "            \n",
    "    print('Number of different words: %d'%len(vocab.word2count))\n",
    "    \n",
    "    return tokenized_dataset\n",
    "\n",
    "dataset_train_tokenized = tokenizeDataset(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last word in vocabulary will be pg&e with a frequency of appearance of 32\n",
      "Vocabulary (size 50000) is built. The firsts words are:\n",
      "UNK (appeared 614062 times)\n",
      ". (appeared 4992479 times)\n",
      "the (appeared 4013086 times)\n",
      ", (appeared 3710445 times)\n",
      "of (appeared 2182732 times)\n"
     ]
    }
   ],
   "source": [
    "vocab.buildVocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([401, 1])\n",
      "torch.Size([28, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def indexesFromSentence(sentence): # sentence is a list of tokens\n",
    "    return [vocab.word2index[word] for word in sentence]\n",
    "        \n",
    "def variableFromSentence(sentence):\n",
    "    indexes = indexesFromSentence(sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return Variable(torch.LongTensor(indexes).view(-1,1))\n",
    "                    \n",
    "def variablesFromPair(pair):\n",
    "    input_var = variableFromSentence(pair[0])\n",
    "    target_var = variableFromSentence(pair[1])\n",
    "    return input_var, target_var\n",
    "        \n",
    "example = random.choice(dataset_train_tokenized)\n",
    "# print(example['passages'])\n",
    "input_var = [item for sublist in example['passages'] for item in sublist]\n",
    "input_var = list(input_var[:400])\n",
    "input_var = variableFromSentence(input_var)\n",
    "print(input_var.size())\n",
    "target_var = example['wellFormedAnswers'][0]\n",
    "target_var = variableFromSentence(target_var)\n",
    "print(target_var.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([401, 1])\n",
      "torch.Size([401, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,embedding_size=EMBEDDING_SIZE,hidden_size=HIDDEN_SIZE,voc_size=VOC_SIZE):\n",
    "        super(EncoderLSTM,self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.voc_size = voc_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(voc_size, embedding_size)\n",
    "        self.bilstm = nn.LSTM(embedding_size, hidden_size, num_layers =1, bidirectional=True)\n",
    "                \n",
    "    def initHidden(self):\n",
    "        return (Variable(torch.zeros(2, 1, self.hidden_size)), # 2 because bidirectional\n",
    "                Variable(torch.zeros(2, 1, self.hidden_size)))\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "#         print(input)\n",
    "        embedded = self.embedding(input).view(1,1,-1)\n",
    "        output, hidden = self.bilstm(embedded, hidden)\n",
    "        return hidden\n",
    "        \n",
    "encoder = EncoderLSTM()\n",
    "input_length = len(input_var)\n",
    "encoder_hidden = encoder.initHidden()        \n",
    "h = Variable(torch.zeros(input_length, encoder.hidden_size*2))        \n",
    "for ei in range(input_length):\n",
    "    encoder_hidden = encoder(input_var[ei],encoder_hidden)\n",
    "    h[ei] = torch.cat((encoder_hidden[0][0],encoder_hidden[1][0]),1)\n",
    "print(input_var.size())\n",
    "print(h.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 11.0971\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class AttnDecoderLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,embedding_size=EMBEDDING_SIZE,hidden_size=HIDDEN_SIZE,output_size=OUTPUT_SIZE,\n",
    "                 voc_size=VOC_SIZE,max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderLSTM,self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.voc_size = voc_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(voc_size, embedding_size)\n",
    "        self.decoder_bilstm = nn.LSTM(self.embedding_size,self.hidden_size, num_layers=1, bidirectional = True)\n",
    "        self.attn_Ws = nn.Linear(hidden_size *2, hidden_size) #?\n",
    "        self.attn_Wh = nn.Linear(hidden_size *2, hidden_size) #?\n",
    "        self.attn_v = nn.Linear(hidden_size, 1)\n",
    "        self.lin_V1 = nn.Linear(hidden_size*4, hidden_size)\n",
    "        self.lin_V2 = nn.Linear(hidden_size, output_size)\n",
    "        self.test = nn.Linear(hidden_size*2,output_size)\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return (Variable(torch.zeros(2, 1, self.hidden_size)), # 2 because bidirectional\n",
    "                Variable(torch.zeros(2, 1, self.hidden_size)))\n",
    "        \n",
    "    def forward_rec(self,input,hidden):\n",
    "        emb = self.embedding(input).view(1,1,-1)\n",
    "        output, hidden = self.decoder_bilstm(emb,hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "\n",
    "    def forward(self, target_var, h):\n",
    "        input_length = h.size()[0]\n",
    "        target_length = len(target_var)\n",
    "        hidden = self.initHidden()        \n",
    "        outputs = Variable(torch.zeros(target_length, self.hidden_size*2))  \n",
    "        s = Variable(torch.zeros(target_length,self.hidden_size*2))\n",
    "        input = Variable(torch.LongTensor([[SOS_token]]))    \n",
    "        \n",
    "        for di in range(target_length):\n",
    "            output, hidden = self.forward_rec(input, hidden)\n",
    "            outputs[di] = output[0]\n",
    "            s[di] = torch.cat((hidden[0][0],hidden[1][0]),1)\n",
    "            input = target_var[di]\n",
    "        \n",
    "        # attention distribution\n",
    "        Wh = self.attn_Wh(h) # dim: # of words in input , 256\n",
    "        Ws = self.attn_Ws(s) # dim: # of words in target , 256\n",
    "        Wh_Ws_d = Variable(torch.zeros(target_length,input_length,256)) # dim: # of words in target, # of words in input , 256 \n",
    "        for i in range(target_length):\n",
    "            Wh_Ws_d[i] = torch.add(Wh,Ws[0])\n",
    "        Wh_Ws_d = F.tanh(Wh_Ws_d) # dim: # of words in target, # of words in input , 256 \n",
    "        e = self.attn_v(Wh_Ws_d) # dim: # of words in target, # of words in input , 1 \n",
    "        e = e.permute(0,2,1) # dim: # of words in target, 1, # of words in input \n",
    "        a = F.softmax(e, dim=2) # dim: # of words in target, 1, # of words in input \n",
    "        h_extended = torch.add(Variable(torch.zeros(target_length,input_length,self.hidden_size*2)),h)\n",
    "        hstar = torch.bmm(a,h_extended) # dim: #of target words, 1, 512\n",
    "        # vocabulary distribution\n",
    "        v1 = torch.cat((s.unsqueeze(1),hstar),dim=2) # dim: #of target words, 1, 1024\n",
    "        v1 = self.lin_V1(v1) # dim: #of target words, 1, 256\n",
    "        v2 = self.lin_V2(v1) # dim: #of target words, 1, vocabulary size\n",
    "        Pvocab = F.softmax(v2,dim=2) # dim: #of target words, 1, vocabulary size\n",
    "        x = self.test(s).unsqueeze(1)\n",
    "#         print(s.size())\n",
    "        Pvocab = F.log_softmax(x,2)\n",
    "        \n",
    "        return Pvocab, outputs    \n",
    "      \n",
    "decoder2 = AttnDecoderLSTM()\n",
    "loss = 0\n",
    "Pvocab, outputs = decoder2(target_var,h)\n",
    "criterion = nn.NLLLoss()\n",
    "for i in range(len(outputs)):\n",
    "    loss += criterion(Pvocab[i],target_var[i])\n",
    "print (loss/len(target_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s/60)\n",
    "    s -= m*60\n",
    "    return '%dm %ds' %(m,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting encoding\n",
      "Starting decoding, target_length = 28\n",
      "Starting backprop\n",
      "Done with backprop, took 0m 1s\n",
      "11.445977347237724\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def trainingStep(input_var, target_var, encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
    "          criterion, max_length=MAX_LENGTH, debug=False):\n",
    "    \n",
    "    if debug:\n",
    "        print('Starting encoding')\n",
    "        \n",
    "    input_length = len(input_var)\n",
    "    encoder_hidden = encoder.initHidden()        \n",
    "    h = Variable(torch.zeros(input_length, encoder.hidden_size*2))        \n",
    "    for ei in range(input_length):\n",
    "        encoder_hidden = encoder(input_var[ei],encoder_hidden)\n",
    "        h[ei] = torch.cat((encoder_hidden[0][0],encoder_hidden[1][0]),1)\n",
    "\n",
    "    target_length = len(target_var)    \n",
    "    if debug:\n",
    "        print('Starting decoding, target_length = %d'%target_length)\n",
    "    \n",
    "    Pvocab, outputs = decoder(target_var,h)\n",
    "    loss = 0\n",
    "    for i in range(target_length):\n",
    "        loss += criterion(Pvocab[i],target_var[i])        \n",
    "        \n",
    "    if debug:\n",
    "        start = time.time()\n",
    "        print('Starting backprop')\n",
    "    loss.backward()    \n",
    "    if debug:\n",
    "        print('Done with backprop, took %s'%timeSince(start))\n",
    "        \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_length\n",
    "\n",
    "\n",
    "encoder_opt = optim.SGD(encoder.parameters(), lr=0.01)\n",
    "decoder_opt = optim.SGD(encoder.parameters(), lr=0.01)\n",
    "decoder = AttnDecoderLSTM()\n",
    "loss = trainingStep(input_var,target_var,encoder,decoder,encoder_opt,decoder_opt,nn.NLLLoss(),debug=True)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses = []\n",
    "\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    print_loss_total = 0 # reset every print_every\n",
    "    plot_loss_total = 0 # reset every plot_every\n",
    "    \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr = learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr = learning_rate)\n",
    "    training = [random.choice(dataset_train_tokenized) for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    for iter in range(1, n_iters + 1):\n",
    "        example = training[iter]\n",
    "        input_variable = [item for sublist in example['passages'] for item in sublist]\n",
    "        input_variable = list(input_variable[:400])\n",
    "        input_variable = variableFromSentence(input_variable)\n",
    "        target_variable = example['wellFormedAnswers'][0]\n",
    "        target_variable = variableFromSentence(target_variable)\n",
    "\n",
    "        loss = trainingStep(input_variable, target_variable, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print(example['wellFormedAnswers'][0])\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "            \n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'fastest', 'moves', 'seismic', 'wave', 'is', 'p', 'wave', 'or', 'primary', 'wave', '.']\n",
      "1m 28s (50 0%) 9.9580\n",
      "['a', 'scramjet', 'or', 'supersonic', 'combusting', 'ramjet', 'is', 'a', 'variant', 'of', 'a', 'ramjet', 'airbreathing', 'jet', 'engine', 'in', 'which', 'combustion', 'takes', 'place', 'in', 'supersonic', 'airflow', '.']\n",
      "2m 50s (100 1%) 27.9004\n",
      "['the', 'average', 'yacht', 'captain', 'salary', 'is', '$', '150', ',', '000', 'a', 'year', '.']\n",
      "4m 10s (150 2%) 15.9316\n",
      "['the', 'max', 'time', 'before', 'you', 'write', 'thank', 'you', 'notes', 'after', 'a', 'funeral', 'is', 'within', 'two', 'to', 'three', 'weeks', '.']\n",
      "5m 24s (200 2%) 38.2521\n",
      "['the', 'causes', 'of', 'pain', 'and', 'swelling', 'of', 'taste', 'buds', 'are', 'mild', 'burn', 'from', 'hot', 'foods', ',', 'spicy', 'foods', ',', 'sensitivity', 'or', 'allergic', 'reaction', 'to', 'spices', 'and', 'other', 'foods', 'and', 'liquids', 'ingested', 'to', 'a', 'number', 'of', 'medical', 'conditions', '.']\n",
      "6m 44s (250 3%) 96.4390\n",
      "['asbestos', 'poisoning', 'is', 'the', 'impact', 'that', 'asbestos', 'exposure', 'can', 'have', 'on', 'humans', '.']\n",
      "7m 46s (300 4%) 46.3433\n",
      "['repose', 'is', 'a', 'formal', 'or', 'literary', 'term', 'used', 'to', 'mean', 'the', 'act', 'of', 'resting', ',', 'or', 'the', 'state', 'of', 'being', 'at', 'rest', '.']\n",
      "9m 7s (350 4%) 55.5309\n",
      "['the', 'average', 'cost', 'to', 'install', 'premium', 'hardwood', 'floor', 'is', 'between', '$', '5', 'and', '$', '10', 'per', 'square', 'foot', '.']\n",
      "10m 28s (400 5%) 121.2346\n",
      "['holly', 'is', 'in', 'oakland', 'county', ',', 'michigan', '.']\n",
      "11m 48s (450 6%) 1225.7815\n",
      "['after', 'fertilization', ',', 'the', 'zygote', 'begins', 'to', 'divide', 'rapidly', '.', 'twenty-four', 'hours', 'after', 'fertilization', ',', 'the', 'zygote', 'contains', 'two', 'cells', ',', 'and', 'three', 'days', 'after', 'fertilization', 'the', 'zygote', 'has', '16', 'cells', 'and', 'is', 'called', 'a', 'morula', '.']\n",
      "13m 6s (500 6%) 2943.8142\n",
      "['it', 'takes', '12', 'to', '14', 'days', 'for', 'a', 'bird', 'egg', 'to', 'hatch', '.']\n",
      "14m 25s (550 7%) 4307.7010\n",
      "['the', 'reese', \"'s\", 'pieces', 'were', 'invented', 'in', '1978', '.']\n",
      "15m 38s (600 8%) 5257.4421\n",
      "['you', 'can', 'keep', 'an', 'insulin', 'bottle', 'for', '28', 'days', 'once', 'it', 'is', 'opened', '.']\n",
      "17m 1s (650 8%) 5891.8999\n",
      "['french', 'montana', 'was', 'born', 'in', 'rabat', ',', 'morocco', '.']\n",
      "17m 58s (700 9%) 6147.3552\n",
      "['the', 'tuition', 'cost', 'of', 'university', 'of', 'iowa', 'is', '$', '6', ',', '678', 'per', 'year', 'for', 'instate', 'residents', '.']\n",
      "19m 13s (750 10%) 6157.0932\n",
      "['exilis', 'is', 'a', 'type', 'of', 'treatment', 'to', 'tighten', 'the', 'skin', '.']\n",
      "20m 30s (800 10%) 5969.1468\n",
      "['you', 'may', 'feel', 'baby', 'movement', 'as', 'early', 'as', '13', 'weeks', 'for', '2nd', 'pregnancy', '.']\n",
      "21m 44s (850 11%) 5559.0791\n",
      "['the', 'definition', 'of', 'an', 'induction', 'is', 'a', 'rite', 'of', 'passage', 'introduction', 'of', 'an', 'individual', 'into', 'a', 'body', 'such', 'as', 'the', 'armed', 'forces', '.']\n",
      "23m 0s (900 12%) 9100.3257\n",
      "['the', 'dogs', 'can', 'hear', '45', 'khz', 'better', 'than', 'humans', '.']\n",
      "24m 13s (950 12%) 9758.5152\n",
      "['oakwood', 'hills', 'is', 'in', 'mchenry', 'county', ',', 'illinois', '.']\n",
      "25m 22s (1000 13%) 4239.1460\n",
      "['the', 'fungus', 'causes', 'the', 'toenail', ',', 'particularly', 'on', 'the', 'big', 'toe', ',', 'to', 'thicken', 'and', 'discolor', ';', 'the', 'end', 'may', 'separate', 'from', 'the', 'nail', 'bed', '.', 'often', ',', 'the', 'nail', 'is', 'so', 'thick', 'you', 'can’t', 'cut', 'it', '.']\n",
      "26m 33s (1050 14%) 4454.1949\n",
      "['the', 'puppy', 'vaccinations', 'cost', '$', '20', 'to', '$', '150', 'in', 'the', 'first', 'year', ',', 'and', 'from', '$', '10', 'to', '$', '100', 'per', 'year', 'afterward', '.']\n",
      "27m 30s (1100 14%) 3640.1246\n",
      "['duncan', 'is', 'a', 'town', 'in', 'spartanburg', 'county', ',', 'south', 'carolina', ',', 'united', 'states', '.']\n",
      "28m 44s (1150 15%) 2457.8096\n",
      "['to', 'get', 'a', 'toned', 'chest', 'in', 'men', ',', 'do', 'cardio', 'three', 'to', 'five', 'days', 'per', 'week', 'for', 'at', 'least', '30', 'minutes', ',', 'train', 'at', 'a', 'moderate', 'to', 'high', 'intensity', 'with', 'activities', 'such', 'as', 'swimming', ',', 'biking', ',', 'running', 'or', 'even', 'jump', 'rope', ',', 'train', 'your', 'chest', 'and', 'abdominal', 'muscle', 'two', 'or', 'three', 'times', 'per', 'week', 'on', 'nonconsecutive', 'days', '.']\n",
      "30m 3s (1200 16%) 1301.0371\n",
      "['the', 'gold', 'coast', 'infrastructure', 'charges', 'are', '$', '28', ',', '000', '.']\n",
      "31m 22s (1250 16%) 1337.8076\n",
      "['bryan', 'baeumler', 'has', 'a', 'net', 'worth', 'of', '$', '15', 'million', '.']\n",
      "32m 37s (1300 17%) 955.5133\n",
      "['the', 'ferric', 'chloride', 'test', 'is', 'for', 'the', 'detection', 'of', 'phenylketonuria', '.']\n",
      "33m 47s (1350 18%) 280.5441\n",
      "['dr', '.', 'jonas', 'salk', 'was', 'credited', 'for', 'discovering', 'the', 'vaccine', 'for', 'polio', '.']\n",
      "35m 2s (1400 18%) 257.3086\n",
      "['a', 'judgement', 'will', 'stay', 'for', '15', 'years', 'on', 'my', 'credit', 'report', '.']\n",
      "36m 8s (1450 19%) 195.0342\n",
      "['rickets', 'disease', 'caused', 'by', 'deficiency', 'of', 'vitamin', 'd', ',', 'calcium', ',', 'or', 'phosphate', '.']\n",
      "37m 2s (1500 20%) 730.1679\n",
      "['the', 'municipal', 'credit', 'union', 'phone', 'number', 'is', '212', '693', '4900', '.']\n",
      "38m 11s (1550 20%) 1074.0367\n",
      "['dallas', 'cowboys', 'revenue', 'is', 'gross', '.']\n",
      "39m 18s (1600 21%) 981.8240\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-676382459c04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mattn_decoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttnDecoderLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mall_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-94c65ec0b028>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         loss = trainingStep(input_variable, target_variable, encoder,\n\u001b[0;32m---> 22\u001b[0;31m                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-05908481b5b2>\u001b[0m in \u001b[0;36mtrainingStep\u001b[0;34m(input_var, target_var, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length, debug)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting backprop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done with backprop, took %s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mtimeSince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder1 = EncoderLSTM()\n",
    "attn_decoder1 = AttnDecoderLSTM()\n",
    "\n",
    "all_losses = trainIters(encoder1, attn_decoder1, 7500, learning_rate=0.001, print_every=50, plot_every = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    plt.plot(points)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2bed8dda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
