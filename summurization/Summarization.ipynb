{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "UNK = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 128\n",
    "HIDDEN_SIZE = 256\n",
    "VOC_SIZE = 50000 #vocab.n_words#50000 #for both source and target\n",
    "OUTPUT_SIZE = VOC_SIZE #10 # ?? \n",
    "BASELINE_VOC_SIZE = VOC_SIZE\n",
    "MAX_LENGTH = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def loadDataset(path,limit=100000000000):\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = list()\n",
    "    i = 0\n",
    "    for line in open(path, 'r'):\n",
    "        all_data = json.loads(line)\n",
    "        for data in all_data:\n",
    "            if i <limit:\n",
    "                dataset.append(data)\n",
    "                i += 1\n",
    "            else:\n",
    "                break\n",
    "    print(\"Loaded %d pieces of data\"%len(dataset))\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Loaded 14261 pieces of data\n",
      "Loading dataset...\n",
      "Loaded 114091 pieces of data\n",
      "Loading dataset...\n",
      "Loaded 3 pieces of data\n"
     ]
    }
   ],
   "source": [
    "path_dev = \"data/msmarco_2wellformed/dev_v2.0_well_formed.json\"\n",
    "path_train = \"data/msmarco_2wellformed/train_v2.0_well_formed.json\"\n",
    "path_eval = \"data/msmarco_2wellformed/evalpublicwellformed.json\"\n",
    "dataset_dev = loadDataset(path_dev)\n",
    "dataset_train = loadDataset(path_train)\n",
    "dataset_eval = loadDataset(path_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "what currency did slovakia use before euro\n",
      "Slovakia used Koruna before the euro.\n",
      "\n",
      "how early should a baby shower be thrown\n",
      "A baby shower should be thrown as early as 4 to 8 weeks before the baby is due, otherwise set 4 to 6 weeks after the babyâ€™s birth.\n",
      "\n",
      "how must does it cost to have a mole removed\n",
      "It costs $150 to $400 to have a mole removed.\n",
      "\n",
      "what does vitamin d and the immune system\n",
      "The immune system needs vitamin D to fight off invading bacteria and viruses.\n",
      "\n",
      "when was nuclear fission discovered\n",
      "Nuclear fission was discovered on December 17, 1938.  \n",
      "\n",
      "what does terrior mean\n",
      "Terrior means a French wine.\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print('')\n",
    "    x = random.choice(dataset_train)\n",
    "    print(x['query'])\n",
    "    print(x['wellFormedAnswers'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'can', \"n't\", 'open', 'the', 'door', 'because', 'the', '30-year', 'old', '/', 'blond-hair', 'guy', 'does', \"n't\", 'want', 'to', 'let', 'me', 'in', '.', 'he', \"'s\", 'mean', ',', 'is', \"n't\", 'he', '?', 'i', 'can', \"n't\", 'go', 'in', '!', 'there', \"'s\", 'no', 'other', 'way', '!']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def insertCharIfSeq(sentence1,c,seq):\n",
    "    i = 0\n",
    "    indexes = [m.start() for m in re.finditer(seq, sentence1)]\n",
    "    for index in indexes:\n",
    "        sentence1 = sentence1[:index+i] + c + sentence1[index+i:]\n",
    "        i += 1\n",
    "    return sentence1\n",
    "\n",
    "def processContractions(sentence1):\n",
    "    sentence1 = insertCharIfSeq(sentence1,\" \",\"'s\")\n",
    "    sentence1 = insertCharIfSeq(sentence1,\" \",\"'m\")\n",
    "    sentence1 = insertCharIfSeq(sentence1,\" \",\"'ll\")\n",
    "    sentence1 = insertCharIfSeq(sentence1,\" \",\"'ve\")\n",
    "    sentence1 = insertCharIfSeq(sentence1,\" \",\"'re\")\n",
    "    sentence1 = insertCharIfSeq(sentence1,\" \",\"'d\")\n",
    "    return sentence1\n",
    "\n",
    "\n",
    "def processNegatives(sentence1):\n",
    "    i = 0\n",
    "    indexes = [m.start() for m in re.finditer(\"can't\", sentence1)]\n",
    "    for index in indexes:\n",
    "        sentence1 = sentence1[:index+i+3] + sentence1[index+i+2:]\n",
    "        i += 1\n",
    "    return insertCharIfSeq(sentence1,\" \",\"n't\")\n",
    "\n",
    "## WHAT TO DO WITH HYPHENS ??\n",
    "\n",
    "def tokenizeSentence(sentence1):\n",
    "#     processFinalPeriod(sentence1)\n",
    "    sentence1 = processContractions(sentence1)\n",
    "    sentence1 = processNegatives(sentence1)\n",
    "    s = sentence1.lower()\n",
    "    s = re.sub('''([.,!\"?$;:/#`()])''', r' \\1 ', s)\n",
    "    s = re.sub('\\s{2,}', ' ', s)\n",
    "    s = s.split()\n",
    "#     s = processHyphenIfUnknownWords(s,glove)\n",
    "#     s.append('</s>')\n",
    "#     s = ['<s>'] + s\n",
    "    return s\n",
    "\n",
    "def testTokenizeSentence():\n",
    "    sentence1 = \"I can't open the door because the 30-year old/blond-hair guy doesn't want to let me in. He's mean, isn't he? I can't go in! There's no other way!\"\n",
    "    words = tokenizeSentence(sentence1)\n",
    "    print(words)\n",
    "    \n",
    "testTokenizeSentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self,voc_size=VOC_SIZE):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"UNK\"}\n",
    "        self.wordsOrderedByFreq = list()\n",
    "        self.word2count[\"UNK\"] = 0\n",
    "        self.n_words = 3\n",
    "        self.voc_size = voc_size\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence:\n",
    "            self.addWord(word)\n",
    "            \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "#             self.index2word[self.n_words] = word\n",
    "#             self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def buildVocabulary(self):\n",
    "        # sort words by frequencies\n",
    "        self.wordsOrderedByFreq = list(reversed(sorted(self.word2count, key=lambda key: self.word2count[key])))\n",
    "        # get voc_size th element in list (= last word in vocab)\n",
    "        last_word = self.wordsOrderedByFreq[self.voc_size]\n",
    "        # get frequency of last word\n",
    "        freq = self.word2count[last_word]\n",
    "        print(\"Last word in vocabulary will be %s with a frequency of appearance of %d\"%(last_word,freq))\n",
    "        for i in range(self.voc_size):\n",
    "            word = self.wordsOrderedByFreq[i]\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        for j in range(self.voc_size,len(self.wordsOrderedByFreq)):\n",
    "            self.word2index[word] = 2\n",
    "            self.n_words += 1\n",
    "            self.word2count['UNK'] += 1\n",
    "        print(\"Vocabulary (size %d) is built. The firsts words are:\"%self.voc_size)\n",
    "        for k in range(2,7):\n",
    "            word = self.index2word[k]\n",
    "            print(\"%s (appeared %d times)\"%(word,self.word2count[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n",
      "Number of different words: 664062\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "vocab = Vocab()\n",
    "\n",
    "def tokenizeDataset(dataset):\n",
    "    tokenized_dataset = list()\n",
    "    len_dataset = len(dataset)\n",
    "    print_every = math.floor(len_dataset / 10)\n",
    "    print('Tokenizing dataset...')\n",
    "    for i, data in enumerate(dataset):\n",
    "        tokenized_data = dict()\n",
    "        tokenized_data['answers'] = list()\n",
    "        tokenized_data['wellFormedAnswers'] = list()\n",
    "        tokenized_data['passages'] = list()\n",
    "        for answer in data['answers']:\n",
    "            t = tokenizeSentence(answer)\n",
    "            tokenized_data['answers'].append(t)\n",
    "            vocab.addSentence(t)\n",
    "        for wf_answer in data['wellFormedAnswers']:\n",
    "            t = tokenizeSentence(wf_answer)\n",
    "            tokenized_data['wellFormedAnswers'].append(t)\n",
    "            vocab.addSentence(t)\n",
    "        for passage in data['passages']:\n",
    "            t = tokenizeSentence(passage['passage_text'])\n",
    "            tokenized_data['passages'].append(t)\n",
    "            vocab.addSentence(t)\n",
    "        if i>0 and i%print_every == 0:\n",
    "            print('... %d%%'%(i/print_every*10),end=\"\\r\")\n",
    "        tokenized_dataset.append(tokenized_data)\n",
    "            \n",
    "    print('Number of different words: %d'%len(vocab.word2count))\n",
    "    \n",
    "    return tokenized_dataset\n",
    "\n",
    "tokenized_data = tokenizeDataset(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last word in vocabulary will be pg&e with a frequency of appearance of 32\n",
      "Vocabulary (size 50000) is built. The firsts words are:\n",
      "UNK (appeared 614062 times)\n",
      ". (appeared 4992479 times)\n",
      "the (appeared 4013086 times)\n",
      ", (appeared 3710445 times)\n",
      "of (appeared 2182732 times)\n"
     ]
    }
   ],
   "source": [
    "vocab.buildVocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([443, 1])\n",
      "torch.Size([13, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def indexesFromSentence(sentence): # sentence is a list of tokens\n",
    "    return [vocab.word2index[word] for word in sentence]\n",
    "        \n",
    "def variableFromSentence(sentence):\n",
    "    indexes = indexesFromSentence(sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return Variable(torch.LongTensor(indexes).view(-1,1))\n",
    "                    \n",
    "def variablesFromPair(pair):\n",
    "    input_var = variableFromSentence(pair[0])\n",
    "    target_var = variableFromSentence(pair[1])\n",
    "    return input_var, target_var\n",
    "        \n",
    "example = random.choice(tokenized_data)\n",
    "# print(example['passages'])\n",
    "input_var = [item for sublist in example['passages'] for item in sublist]\n",
    "input_var = variableFromSentence(input_var)\n",
    "print(input_var.size())\n",
    "target_var = example['wellFormedAnswers'][0]\n",
    "target_var = variableFromSentence(target_var)\n",
    "print(target_var.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([443, 1])\n",
      "torch.Size([443, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,embedding_size=EMBEDDING_SIZE,hidden_size=HIDDEN_SIZE,voc_size=VOC_SIZE):\n",
    "        super(EncoderLSTM,self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.voc_size = voc_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(voc_size, embedding_size)\n",
    "        self.bilstm = nn.LSTM(embedding_size, hidden_size, num_layers =1, bidirectional=True)\n",
    "                \n",
    "    def initHidden(self):\n",
    "        return (Variable(torch.zeros(2, 1, self.hidden_size)), # 2 because bidirectional\n",
    "                Variable(torch.zeros(2, 1, self.hidden_size)))\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "#         print(input)\n",
    "        embedded = self.embedding(input).view(1,1,-1)\n",
    "        output, hidden = self.bilstm(embedded, hidden)\n",
    "        return hidden\n",
    "        \n",
    "encoder = EncoderLSTM()\n",
    "input_length = len(input_var)\n",
    "encoder_hidden = encoder.initHidden()        \n",
    "h = Variable(torch.zeros(input_length, encoder.hidden_size*2))        \n",
    "for ei in range(input_length):\n",
    "    encoder_hidden = encoder(input_var[ei],encoder_hidden)\n",
    "    h[ei] = torch.cat((encoder_hidden[0][0],encoder_hidden[1][0]),1)\n",
    "print(input_var.size())\n",
    "print(h.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 512])\n",
      "Variable containing:\n",
      "1.00000e-05 *\n",
      " -1.9482\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class AttnDecoderLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,embedding_size=EMBEDDING_SIZE,hidden_size=HIDDEN_SIZE,output_size=OUTPUT_SIZE,\n",
    "                 voc_size=VOC_SIZE,max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderLSTM,self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.voc_size = voc_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(voc_size, embedding_size)\n",
    "        self.decoder_bilstm = nn.LSTM(self.embedding_size,self.hidden_size, num_layers=1, bidirectional = True)\n",
    "        self.attn_Ws = nn.Linear(hidden_size *2, hidden_size) #?\n",
    "        self.attn_Wh = nn.Linear(hidden_size *2, hidden_size) #?\n",
    "        self.attn_v = nn.Linear(hidden_size, 1)\n",
    "        self.lin_V1 = nn.Linear(hidden_size*4, hidden_size)\n",
    "        self.lin_V2 = nn.Linear(hidden_size, output_size)\n",
    "        self.test = nn.Linear(hidden_size*2,output_size)\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return (Variable(torch.zeros(2, 1, self.hidden_size)), # 2 because bidirectional\n",
    "                Variable(torch.zeros(2, 1, self.hidden_size)))\n",
    "        \n",
    "    def forward_rec(self,input,hidden):\n",
    "        emb = self.embedding(input).view(1,1,-1)\n",
    "        output, hidden = self.decoder_bilstm(emb,hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "\n",
    "    def forward(self, target_var, h):\n",
    "        input_length = h.size()[0]\n",
    "        target_length = len(target_var)\n",
    "        hidden = self.initHidden()        \n",
    "        outputs = Variable(torch.zeros(target_length, self.hidden_size*2))  \n",
    "        s = Variable(torch.zeros(target_length,self.hidden_size*2))\n",
    "        input = Variable(torch.LongTensor([[SOS_token]]))    \n",
    "        \n",
    "        for di in range(target_length):\n",
    "            output, hidden = self.forward_rec(input, hidden)\n",
    "            outputs[di] = output[0]\n",
    "            s[di] = torch.cat((hidden[0][0],hidden[1][0]),1)\n",
    "            input = target_var[di]\n",
    "        \n",
    "        # attention distribution\n",
    "        Wh = self.attn_Wh(h) # dim: # of words in input , 256\n",
    "        Ws = self.attn_Ws(s) # dim: # of words in target , 256\n",
    "        Wh_Ws_d = Variable(torch.zeros(target_length,input_length,256)) # dim: # of words in target, # of words in input , 256 \n",
    "        for i in range(target_length):\n",
    "            Wh_Ws_d[i] = torch.add(Wh,Ws[0])\n",
    "        Wh_Ws_d = F.tanh(Wh_Ws_d) # dim: # of words in target, # of words in input , 256 \n",
    "        e = self.attn_v(Wh_Ws_d) # dim: # of words in target, # of words in input , 1 \n",
    "        e = e.permute(0,2,1) # dim: # of words in target, 1, # of words in input \n",
    "        a = F.softmax(e, dim=2) # dim: # of words in target, 1, # of words in input \n",
    "        h_extended = torch.add(Variable(torch.zeros(target_length,input_length,self.hidden_size*2)),h)\n",
    "        hstar = torch.bmm(a,h_extended) # dim: #of target words, 1, 512\n",
    "        # vocabulary distribution\n",
    "        v1 = torch.cat((s.unsqueeze(1),hstar),dim=2) # dim: #of target words, 1, 1024\n",
    "        v1 = self.lin_V1(v1) # dim: #of target words, 1, 256\n",
    "        v2 = self.lin_V2(v1) # dim: #of target words, 1, vocabulary size\n",
    "        Pvocab = F.softmax(v2,dim=2) # dim: #of target words, 1, vocabulary size\n",
    "        x = self.test(s).unsqueeze(1)\n",
    "        print(s.size())\n",
    "        Pvocab = F.softmax(x,2)\n",
    "        \n",
    "        return Pvocab, outputs    \n",
    "      \n",
    "decoder2 = AttnDecoderLSTM2()\n",
    "loss = 0\n",
    "Pvocab, outputs = decoder2(target_var,h)\n",
    "criterion = nn.NLLLoss()\n",
    "for i in range(len(outputs)):\n",
    "    loss += criterion(Pvocab[i],target_var[i])\n",
    "print (loss/len(target_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s/60)\n",
    "    s -= m*60\n",
    "    return '%dm %ds' %(m,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting encoding\n",
      "Starting decoding, target_length = 13\n",
      "torch.Size([13, 512])\n",
      "Starting backprop\n",
      "Done with backprop, took 0m 0s\n",
      "-1.9825860643042968e-05\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def trainingStep(input_var, target_var, encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
    "          criterion, max_length=MAX_LENGTH, debug=False):\n",
    "    \n",
    "    if debug:\n",
    "        print('Starting encoding')\n",
    "        \n",
    "    input_length = len(input_var)\n",
    "    encoder_hidden = encoder.initHidden()        \n",
    "    h = Variable(torch.zeros(input_length, encoder.hidden_size*2))        \n",
    "    for ei in range(input_length):\n",
    "        encoder_hidden = encoder(input_var[ei],encoder_hidden)\n",
    "        h[ei] = torch.cat((encoder_hidden[0][0],encoder_hidden[1][0]),1)\n",
    "\n",
    "    target_length = len(target_var)    \n",
    "    if debug:\n",
    "        print('Starting decoding, target_length = %d'%target_length)\n",
    "    \n",
    "    Pvocab, outputs = decoder(target_var,h)\n",
    "    loss = 0\n",
    "    for i in range(target_length):\n",
    "        loss += criterion(Pvocab[i],target_var[i])        \n",
    "        \n",
    "    if debug:\n",
    "        start = time.time()\n",
    "        print('Starting backprop')\n",
    "    loss.backward()    \n",
    "    if debug:\n",
    "        print('Done with backprop, took %s'%timeSince(start))\n",
    "        \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_length\n",
    "\n",
    "\n",
    "encoder_opt = optim.SGD(encoder.parameters(), lr=0.01)\n",
    "decoder_opt = optim.SGD(encoder.parameters(), lr=0.01)\n",
    "decoder = AttnDecoderLSTM2()\n",
    "loss = trainingStep(input_var,target_var,encoder,decoder,encoder_opt,decoder_opt,nn.NLLLoss(),debug=True)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting encoding\n",
      "Starting decoding, target_length = 27\n",
      "Starting backprop\n",
      "Done with backprop, took 0m 46s\n",
      "-5.813038485400655e-06\n"
     ]
    }
   ],
   "source": [
    "def trainingStep4(input_var, target_var, encoder, decoder4, attention, encoder_optimizer, decoder_optimizer, \n",
    "                  attention_optimizer, criterion, max_length=MAX_LENGTH, debug=False):\n",
    "    \n",
    "    if debug:\n",
    "        print('Starting encoding')\n",
    "        \n",
    "    input_length = len(input_var)\n",
    "    encoder_hidden = encoder.initHidden()        \n",
    "    h = Variable(torch.zeros(input_length, encoder.hidden_size*2))        \n",
    "    for ei in range(input_length):\n",
    "        encoder_hidden = encoder(input_var[ei],encoder_hidden)\n",
    "        h[ei] = torch.cat((encoder_hidden[0][0],encoder_hidden[1][0]),1)\n",
    "\n",
    "    target_length = len(target_var)    \n",
    "    if debug:\n",
    "        print('Starting decoding, target_length = %d'%target_length)\n",
    "    \n",
    "    \n",
    "    outputs = Variable(torch.zeros(target_length, decoder4.hidden_size*2))  \n",
    "    s = Variable(torch.zeros(target_length,decoder4.hidden_size*2))\n",
    "    input = Variable(torch.LongTensor([[SOS_token]]))  \n",
    "    hidden = decoder4.initHidden()  \n",
    "\n",
    "    for di in range(target_length):\n",
    "        output, hidden = decoder4(input, hidden)\n",
    "        outputs[di] = output[0]\n",
    "        s[di] = torch.cat((hidden[0][0],hidden[1][0]),1)\n",
    "        input = target_var[di]\n",
    "    \n",
    "    attention = Attention()\n",
    "    pvocab = attention(h,s)\n",
    "    loss = 0\n",
    "    for i in range(len(outputs)):\n",
    "        loss += criterion(pvocab[i],target_var[i])\n",
    "\n",
    "    if debug:\n",
    "        start = time.time()\n",
    "        print('Starting backprop')\n",
    "    loss.backward()    \n",
    "    if debug:\n",
    "        print('Done with backprop, took %s'%timeSince(start))\n",
    "        \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_length\n",
    "\n",
    "\n",
    "encoder_opt = optim.SGD(encoder.parameters(), lr=0.01)\n",
    "decoder_opt = optim.SGD(encoder.parameters(), lr=0.01)\n",
    "attn_opt = optim.SGD(encoder.parameters(), lr=0.01)\n",
    "loss = trainingStep4(input_var,target_var,encoder,decoder4,attention,encoder_opt,decoder_opt,attn_opt,nn.NLLLoss(),debug=True)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
