{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## Helper class to create the one-hot vectors for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0 #start of sentence\n",
    "EOS_token = 1 #end of sentence\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "            \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess files\n",
    "\n",
    "Transform unicode in ascii and convert to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file\n",
    "\n",
    "Split line into pairs (english / other language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse = False):\n",
    "    print(\"Reading lines...\")\n",
    "    lines = open ('data/%s-%s/%s.txt'%(lang1, lang2, lang1), encoding = 'utf-8').read().strip().split('\\n')\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "    else:\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make it easier\n",
    "\n",
    "Keep only sort sentences that translate to the form \"I am\", \"He is\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = ('i am ', 'i m ','you are ', 'you re ', 'he is ', ' he s ', 'she is ', 'she s ', 'we are ', 'we re ', 'they are ', 'they re ')\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH and p[1].startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 149861 sentence pairs\n",
      "Trimmed to 10535 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4279\n",
      "eng 2737\n",
      "['nous sommes impuissants .', 'we re helpless .']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def prepareData(lang1, lang2, reverse = False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print('Read %d sentence pairs'%len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print('Trimmed to %d sentence pairs'%len(pairs))\n",
    "    print('Counting words...')\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print('Counted words:')\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('fra', 'eng', True)\n",
    "example = random.choice(pairs)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ['nous sommes fatigues .', 'we re helpless .']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2seq model\n",
    "\n",
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "USE_CUDA = False\n",
    "INPUT_SIZE = input_lang.n_words\n",
    "HIDDEN_SIZE = 256\n",
    "OUTPUT_SIZE = output_lang.n_words\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1,1,-1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if USE_CUDA:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "        \n",
    "encoder = EncoderRNN(INPUT_SIZE,HIDDEN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.2406 -0.3146 -0.1436  ...  -0.2661 -0.0426  0.2582\n",
      "-0.4276 -0.5299  0.2215  ...   0.2187  0.1916  0.5870\n",
      "-0.3860 -0.1503  0.0640  ...   0.0087 -0.0737  0.1661\n",
      "          ...             â‹±             ...          \n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 10x256]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_hidden = encoder.initHidden()\n",
    "input_length = input_var.size()[0]\n",
    "encoder_outputs = Variable(torch.zeros(MAX_LENGTH, encoder.hidden_size))\n",
    "encoder_outputs = encoder_outputs.cuda() if USE_CUDA else encoder_outputs\n",
    "for ei in range(input_length):\n",
    "    encoder_output, encoder_hidden = encoder(input_var[ei], encoder_hidden)\n",
    "    encoder_outputs[ei] = encoder_output[0][0]     \n",
    "print(encoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size,hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size,hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self,input,hidden):\n",
    "        output = self.embedding(input).view(1,1,-1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]),dim=1)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(1,1,self.hidden_size))\n",
    "        if USE_CUDA:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-5\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p = 0.1, max_length = MAX_LENGTH):\n",
    "        super(AttnDecoderRNN,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attn = nn.Linear(hidden_size *2, max_length)\n",
    "        self.attn_combine = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size,output_size)\n",
    "        \n",
    "    def forward(self,input,prev_hidden,encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1,1,-1)\n",
    "        embedded = self.dropout(embedded) #1, 1, 256\n",
    "        # prev_hidden: 1, 1, 256\n",
    "        attn_weights = self.attn(torch.cat([embedded,prev_hidden],2)) #1, 1, 10\n",
    "        attn_weights = F.softmax(attn_weights,dim=2) #1, 1, 10\n",
    "        encoder_outputs = encoder_outputs.unsqueeze(0) #1, 10, 256\n",
    "        attn_applied = torch.bmm(attn_weights,encoder_outputs) # 1, 1, 256\n",
    "        output = self.attn_combine(torch.cat([embedded,attn_applied],2)) #1, 1, 256\n",
    "        output = F.relu(output) #1, 1, 256\n",
    "        output, hidden = self.gru(output,prev_hidden) #1, 1, 256 ; 1, 1, 256\n",
    "        output = self.out(output)\n",
    "        output = F.softmax(output,dim=1)\n",
    "        return output, hidden, attn_weights #1, 1, 256 ; 1, 1, 256 ; 1, 1, 10\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1,1,self.hidden_size))\n",
    "        if USE_CUDA:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "        \n",
    "decoder = AttnDecoderRNN(HIDDEN_SIZE,OUTPUT_SIZE)\n",
    "decoder_hidden = encoder_hidden\n",
    "target_length = target_var.size()[0]\n",
    "loss = 0\n",
    "decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "decoder_input = decoder_input.cuda() if USE_CUDA else decoder_input\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Feed the target as the next input\n",
    "for di in range(target_length):\n",
    "    decoder_output, decoder_hidden, decoder_attn = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "    loss += criterion(decoder_output[0], target_var[di])\n",
    "    decoder_input = target_var[di]\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Prepare data\n",
    "\n",
    "Get one hot vectors for each word in sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang,sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def variableFromSentence(lang,sentence):\n",
    "    indexes = indexesFromSentence(lang,sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    result = Variable(torch.LongTensor(indexes).view(-1,1))\n",
    "    if USE_CUDA:\n",
    "        return result.cuda()\n",
    "    else:\n",
    "        return result\n",
    "    \n",
    "def variablesFromPair(pair):\n",
    "    input_variable = variableFromSentence(input_lang,pair[0])\n",
    "    target_variable = variableFromSentence(output_lang,pair[1])\n",
    "    return (input_variable, target_variable)\n",
    "\n",
    "input_var, target_var = variablesFromPair(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEACHER_FORCING_RATIO = 0.5\n",
    "import torch.optim as optim\n",
    "\n",
    "def train(input_var, target_var, encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
    "          criterion, max_length=MAX_LENGTH):\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_var.size()[0]\n",
    "    target_length = target_var.size()[0]\n",
    "    \n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if USE_CUDA else encoder_outputs\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_var[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0][0]\n",
    "        \n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_input = decoder_input.cuda() if USE_CUDA else decoder_input\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < TEACHER_FORCING_RATIO else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        # Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attn = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output[0], target_var[di])\n",
    "            decoder_input = target_var[di]\n",
    "            \n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attn = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output[0], target_var[di])\n",
    "            topv, topi = decoder_output[0].data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "            decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "            decoder_input = decoder_input.cuda() if USE_CUDA else decoder_input\n",
    "            if ni == EOS_token:\n",
    "                break\n",
    "                \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / target_length\n",
    "\n",
    "encoder_opt = optim.SGD(encoder.parameters(), lr=0.01)\n",
    "decoder_opt = optim.SGD(encoder.parameters(), lr=0.01)\n",
    "train(input_var,target_var,encoder,decoder,encoder_opt,decoder_opt,nn.NLLLoss())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep track of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s/60)\n",
    "    s -= m*60\n",
    "    return '%dm %ds' % (m,s)\n",
    "\n",
    "def timeSince(since,percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s),asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fid, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 37s (- 14m 40s) (1000 10%) -1.0000\n",
      "3m 15s (- 13m 0s) (2000 20%) -1.0000\n"
     ]
    }
   ],
   "source": [
    "def trainIters(encoder,decoder,n_iters,learning_rate=0.01,print_every=1000,plot_every=100):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0 # reset every print_every\n",
    "    plot_loss_total = 0 # reset every plot_every\n",
    "    \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr = learning_rate)\n",
    "    decoder_optimizer = optim.SGD(encoder.parameters(), lr = learning_rate)\n",
    "    training_pairs = [variablesFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_variable = training_pair[0]\n",
    "        target_variable = training_pair[1]\n",
    "        \n",
    "        loss = train(input_variable,target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),iter, iter / n_iters *100, print_loss_avg))\n",
    "            \n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = print_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_avg = 0\n",
    "            \n",
    "    showPlot(plot_losses)\n",
    "    \n",
    "trainIters(encoder,decoder,10000,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
